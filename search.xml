<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Word Embedding]]></title>
    <url>%2F2018%2F06%2F05%2FWord-Embedding%2F</url>
    <content type="text"><![CDATA[词向量（Word embedding, 词嵌入）词向量(Word Embedding)，按照字面意思应该是”词嵌入“，或称分布式表示(Distributed Representation) 「其实这三种描述方式中，Distributed Representation是一种神经网络生成词向量的方式」。说到词向量，不少人可能会首先想到Google的Word2Vec「这个太出名」。另外，用Keras之类的框架还有一个Embedding层，也说是将词ID映射为向量。由于先入为主的意识，大家可能就会将词向量跟Word2Vec等同起来，而反过来问“Embedding是哪种词向量？”这类问题，尤其是对于初学者来说，应该是很混淆的。事实上，哪怕对于老手，也不一定能够很好地说清楚。 这一切，还得从one-hot说起… 1. One-hotOne-hot(One-hot编码)也称作一位有效编码，即在一个向量表示中只有一个分量是有效的，例如“学校就在前方”对应的one-hot编码即可表示为： \begin{array}{c|c} \hline \text{学} & [1, 0, 0, 0, 0, 0]\\ \text{校} & [0, 1, 0, 0, 0, 0]\\ \text{就} & [0, 0, 1, 0, 0, 0]\\ \text{在} & [0, 0, 0, 1, 0, 0]\\ \text{前} & [0, 0, 0, 0, 1, 0]\\ \text{方} & [0, 0, 0, 0, 0, 1]\\ \hline \end{array}那么对应的“学校”这个词就可以用如下字向量构成的矩阵对应表示： \begin{pmatrix}1 & 0 & 0 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 & 0 & 0 \end{pmatrix}看到这儿，大家可能感觉到问题了，有多少个字，就得有多少维向量，假如有1万字，那么每个字向量就是1万维（但常用的字不多，可能也就几千个左右，但是按照词的概念来看，常用的词可能就有十几万了） 1#这时候，我们在有的地方会看到有些初学者这样说的，甚至不少专家也是这样说的:"这时候我们就考虑出来了“连续向量表示”，比如用100维的实数向量来表示一个字，这样就大大降低了维度，降低了过拟合的风险，降低了运算量等等" 其实稍微一想也应该知道：稀疏矩阵是更便于计算的呀，甚至在机器学习中有不少地方专门为了将数据矩阵稀疏化来降低运算量。稍微举个例子： \begin{pmatrix}1 & 0 & 0 & 0 & 0 & 0\\ 0 & 1 & 0 & 0 & 0 & 0 \end{pmatrix} \begin{pmatrix}w{11} & w{12} & w{13} \\ w{21} & w{22} & w{23}\\ w{31} & w{32} & w{33}\\ w{41} & w{42} & w{43}\\ w{51} & w{52} & w{53}\\ w{61} & w{62} & w{63}\end{pmatrix} = \begin{pmatrix}w{11} & w{12} & w{13}\\ w{21} & w{22} & w{23}\end{pmatrix}左边的形式表明，这是一个以2x6的one-hot矩阵的为输入、中间层节点数为3的全连接神经网络层，但你看右边，不就相当于在w~ij~这个矩阵中，取出第1、2行，这不是跟所谓的字向量的查表（从表中找出对应字的向量）是一样的吗？事实上，正是如此，这就是所谓的Embedding层，Embedding层就是以one-hot为输入、中间层节点为字向量维数的全连接层，而这个全连接层的参数，就是一个“字向量表”。 12&gt; #One-hot会导致“词汇鸿沟”现象：即任意两个词之间都是孤立的，单单从这两个向量中看不出两个词是否有关系，哪怕是“话筒”和“麦克“这样的同义词也不能幸免于难。&gt; 2. 词向量表示以上举例对应的是“字”在“句子”中的向量表示，而我们一般在实际中考虑的是“词”在“文档”中的向量表示 ( 其中”文档”不一定是实际意义上的文档——可能是以段落划分成，也可能以某个特殊的约定方式将给定的文本数据进行划分「eg：一个章节etc..」) ，还是举两个例子吧，首先举个简单的中文例子： 123“地上”表示为 [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 …]“有”表示为 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 …]“话筒”表示为 [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 …] 进一步地，”地上有话筒”可以表示为： \begin{pmatrix}0&0 &0 &1& 0& 0 &0 &0&0& 0 &0& 0& 0& 0& 0 &0& …\\ 0& 0& 0& 0& 0& 0& 0& 0& 0& 0& 0& 0 &0& 0 &0& 1 &…\\0 &0& 0 &0 &0 &0& 0& 0& 1& 0& 0& 0& 0 &0 &0 &0& … \end{pmatrix}再比如对于现有四个文本： 1234&quot;I come to China to travel&quot;, &quot;This is a car polupar in China&quot;, &quot;I love tea and Apple &quot;, &quot;The work is to write some papers in science&quot; 转换成词向量模式（可通过sklearn的CountVectorizer类来完成，这个类可以帮我们完成文本的词频统计与向量化 — 对应代码如下）， 123456789from sklearn.feature_extraction.text import CountVectorizer vectorizer=CountVectorizer()corpus=["I come to China to travel", "This is a car polupar in China", "I love tea and Apple ", "The work is to write some papers in science"] #print vectorizer.fit_transform(corpus) #这步分别看各个词的统计词频print vectorizer.fit_transform(corpus).toarray()print vectorizer.get_feature_names() 可以得到”每个文本的词向量特征”以及”各个特征代表的词”如下输出： 123456&gt; [[0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 2 1 0 0]&gt; [0 0 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0]&gt; [1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0]&gt; [0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 1 0 1 1]]&gt; [u&apos;and&apos;, u&apos;apple&apos;, u&apos;car&apos;, u&apos;china&apos;, u&apos;come&apos;, u&apos;in&apos;, u&apos;is&apos;, u&apos;love&apos;, u&apos;papers&apos;, u&apos;polupar&apos;, u&apos;science&apos;, u&apos;some&apos;, u&apos;tea&apos;, u&apos;the&apos;, u&apos;this&apos;, u&apos;to&apos;, u&apos;travel&apos;, u&apos;work&apos;, u&apos;write&apos;]&gt; 可以看到我们一共有19个词，所以4个文本都是19维的特征向量。而每一维的向量依次对应了下面的19个词。另外由于词”I”在英文中是停用词，不参加词频的统计。 由于大部分的文本都只会使用词汇表中的很少一部分的词，因此我们的词向量中会有大量的0。也就是说词向量是稀疏的。在实际应用中一般使用稀疏矩阵来存储。 将文本做了词频统计后，我们一般会通过TF-IDF进行词特征值修订，这部分我们后面再讲。 向量化的方法很好用，也很直接，但是在有些场景下很难使用，比如分词后的词汇表非常大，很容易就达到100w+，此时如果我们直接使用向量化的方法，将对应的样本对应特征矩阵载入内存，有可能将内存撑爆，在这种情况下我们怎么办呢？第一反应是我们要进行特征降维，说的没错！而Hash Trick就是非常常用的文本特征降维方法。 3. 词向量的生成方式 词向量的生成通常和语言模型LM绑定在一起。 1. 一种最简单的词向量方式是：One-hot representation； 2. 另一种：Distributed representation ，最早是 Hinton 于 1986 年提出，它可以克服one-hot的两个缺点「1.维数灾难；2.词汇鸿沟」，其基本想法是： 通过训练将某种语言中的每一个词映射成一个固定长度的短向量（当然这里的“短”是相对于 one-hot representation 的“长”而言的），将所有这些向量放在一起形成一个词向量空间，而每一向量则为该空间中的一个点，在这个空间上引入“距离”，则可以根据词之间的距离来判断它们之间的（词法、语义上的）相似性了。 基于NNLM神经网络的早期经典语言模型：用神经网络来训练语言模型的思想最早由百度 IDL （深度学习研究院）的徐伟提出。 这方面最经典的文章要数 Bengio 于 2003 年发表在 JMLR 上的 A Neural Probabilistic Language Model； C&amp;W(Ronan Collobert &amp; Jason Weston)的语言模型：其对应开源了系统SENNA； M&amp;H(Andriy Mnih 和 Geoffrey Hinton)的： LBL(Log-Bilinear)语言模型； HLBL(hierarchical log-bilinear)语言模型； Mikolov 的 RNNLM； Google提出的Word2Vec 这里还要说明一下“Distributional Representation“和”Distributed Representation”的区别。]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Machine Learning</tag>
        <tag>Word PreProcess</tag>
      </tags>
  </entry>
</search>
